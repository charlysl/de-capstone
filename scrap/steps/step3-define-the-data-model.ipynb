{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a697d341",
   "metadata": {},
   "source": [
    "# Step 3: Define the Data Model\n",
    "\n",
    "### The data model is organized as a star schema\n",
    "- The goal is to **trade storage space and consistency checks for speed**.\n",
    "- The key is for queries to perform **as few joins as possible**, which are an expensive but unavoidable operation.\n",
    "- For this reason, dimension schemas will contain all columns relevant to the dimension at hand, even if this **information is repeated** in other dimensions, rather than normalizing.\n",
    "\n",
    "### Design principles that I followed\n",
    "- *Pure star schema*, as opposed to a snowflake schema. There will be a lot of repition, for the reason stated above, given that, as it is, storage is not an issue.\n",
    "- I will use a dimension's natural key, if any, as its *primary key* if the key is derived from a single data source. This principle also applies to compound primariy keys (for instance, in the ```route_dim``` table).\n",
    "- All dimension columns whose values are codes or abbreviations will have a corresponding *descriptive column* (with the ```_desc``` suffix).\n",
    "- *No measures in dimension tables*. So, I used demographics and temperature tables only to derive static population, ethnical and weather categories.\n",
    "- Include both numerical and descriptive columns for every cathegorical variable, with the number reflecting the desired order if it is an ordinal cathegorical variable. The reason is that the whole point of a star schema is to make analysis as easy as possible, and this way the schema can cater to both business users (who prefer descriptive values) and data scientists (who might need numerical variables for some visualizations and features).\n",
    "    - The column naming convention I follow is ```<column name>``` for the description columns and ```<column name>_id``` for their numerical codes. The reason is to have an every day name aimed at business users, whereas I would expect data scientists to deal with the less natural labels.\n",
    "- Do not use *null* values as foreign key values in the fact tables, nor in dimension columns (see [Null Attributes in Dimensions](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/null-dimension-attribute/)). It is OK to use them in fact table measure values (see [Nulls in Fact Tables](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/fact-table-null/))\n",
    "    - any immigration row that is a nk in a dimension will leftanti with that dim table, even if it already exitst in said dim table\n",
    "    - it is very important in cleaning to ensure that values in columns that are in any dimension's natural key are **never** null, either fill them in or replace them with a null marker or drop them, but never leave them as null\n",
    "- For columns that represent *ordinal categorical variables*, their ids' ordering should reflect it.\n",
    "- Do not use monotonically_increasing_int to generate SKs\n",
    "    - they will not be unique if leftanti joining immigration with a dim, ids from one month will be reused in the next, maybe because the id is based on the immigration partition, which can only be on the left of the join, which will often be the same, with new dim ids generated again starting from 0 but offset from an immigration partition\n",
    "    - use md5 instead, low collision risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881ad858",
   "metadata": {},
   "source": [
    "## Model description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db3f42c",
   "metadata": {},
   "source": [
    "### Abstract data model\n",
    "\n",
    "I will start by presenting what I consider as the core entities that are relevant to my understanding of this model's domain:\n",
    "\n",
    "![Conceptual Data Model](images/de-capstone-abstract-data-model.png)\n",
    "\n",
    "**NOTE**: I preferred to draw a more abstract and simpler (fewer symbols) conceptual data model diagram instead of the more common E-R diagram, because in my opinion the latter includes detail that in my opinion is unnecessary when one is already set on a star model. The intention is to highlight the main atoms and how they they are related. The multiplicities represent business rules and will be useful later on when designing data validation.\n",
    "\n",
    "### Core entities description\n",
    "\n",
    "I will describe only those entities whose precise meaning might not be obvious.\n",
    "\n",
    "#### Route\n",
    "A particular airline and flight number combination that flies to a given immigration port city.\n",
    "- it is possible for an airplane to call at multiple US airports, but under the same flight number (see [Can two flights of the same airlines have the same number?](https://www.quora.com/Can-two-flights-of-the-same-airlines-have-the-same-number)).\n",
    "\n",
    "#### Flight\n",
    "An airplane on a particular route that arrived at a given immigration port in a given day.\n",
    "\n",
    "#### Foreign Visitor\n",
    "A non-US citizen that arrived in a flight for non-immigration purposes, such as pleasure, business or studies.\n",
    "- it is possible that there actually were other passengers in a flight, US citizens, but we don't have any data about these and are out of scope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940820ea",
   "metadata": {},
   "source": [
    "### Star schema\n",
    "\n",
    "![Star schema](images/de-capstone-star-schema.png)\n",
    "\n",
    "#### Tables\n",
    "\n",
    "##### flight_fact table\n",
    "\n",
    "The only fact table in this model, it contains aggregate measures of a flight's foreign visitors.\n",
    "\n",
    "The main measure, ```num_visitors```, is the count of each of the resulting foreign visitor categories after grouping by different visitor categories.\n",
    "\n",
    "Each ```time_id``` and ```route_id``` combination refers to an individual flight, which may have multiple foreign ```visitor_id```s, one for each passenger category combination in that particular flight.\n",
    "\n",
    "##### foreign_visitor_dim table\n",
    "Dimension table that represents combinations of foreign visitor categories.\n",
    "\n",
    "##### time_dim table\n",
    "\n",
    "Dimension table that represents time with day granularity.\n",
    "\n",
    "##### route_dim table\n",
    "\n",
    "Dimension table that represents a route, with many columns that describe the destination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817fdf08",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74a9f9d",
   "metadata": {},
   "source": [
    "TODO: high level architecture diagram as in https://medium.com/deutsche-telekom-gurgaon/etl-data-pipeline-and-data-reporting-using-airflow-spark-livy-and-athena-for-oneapp-6d081a419adc\n",
    "- explain the different Airflow tasks in a list as in the link above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53c67ab",
   "metadata": {},
   "source": [
    "![Data Flow diagram](images/data_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc2852",
   "metadata": {},
   "source": [
    "Goal:\n",
    "- a datalake with given star schema as parquet in S3\n",
    "    - vs Redshift, less expensive, need to pay only for cheap object storage\n",
    "    - pay for processing only on demand\n",
    "- to do all all ETL in Airflow and PySpark ...\n",
    "- ... accross multiple Airlfow tasks for visibility ...\n",
    "- ... on a single Spark Cluster ...\n",
    "- ... while passing intermediate in RAM Spark DataFrames between Airflow tasks ...\n",
    "\n",
    "Solution:\n",
    "\n",
    "- Divide the pipeline is multiple *[SparkSubmitOperator](https://airflow.apache.org/docs/apache-airflow/1.10.12/_api/airflow/contrib/operators/spark_submit_operator/index.html)* tasks\n",
    "    - this will allow for fine grained visibility in the Airflow UI\n",
    "    - the drawback is that the data can't flow through main memory\n",
    "    - ETL code must be pyspark 2.4, otherwise incompatible with Airflow v???\n",
    "        - in a way, using Spark SQL might be superior to Data Frames, the former being more portable between Spark versions (i.e ```trim```)\n",
    "    - [Passing dependencies to python spark jobs](https://stackoverflow.com/questions/38066318/whats-the-difference-between-archives-files-py-files-in-pyspark-job-argum)\n",
    "\n",
    "NOT Use **[Apache Livy](https://livy.apache.org/)**.\n",
    "- as in [ETL Data Pipeline and Data Reporting using Airflow, Spark, Livy and Athena for OneApp](https://medium.com/deutsche-telekom-gurgaon/etl-data-pipeline-and-data-reporting-using-airflow-spark-livy-and-athena-for-oneapp-6d081a419adc)\n",
    "\n",
    "EMR Cluster\n",
    "- use Boto3 library inside Airflow task to manage it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24accf2d",
   "metadata": {},
   "source": [
    "#### Spark jobs design\n",
    "```\n",
    "main\n",
    " |__ plugins\n",
    "         |___ etl_spark_jobs\n",
    "                         |___ etl_*.py\n",
    "```\n",
    "- Most Airflow tasks will end up submitting a job to a Spark cluster.\n",
    "- Executed by ```ETLSparkOperator```.\n",
    "- To be testable, they must not execute on import.\n",
    "    - They all have a trampoline function at the end, named with a ```_trampoline``` suffix.\n",
    "    - The trampoline function is executed by a ```__name__ == '__main__'``` when the job is submitted to Spark.\n",
    "- Must be **idempotent**, and this requirement is, by default, and will be assumed without any need to explicitly state it, part of every job's specification, and, hence, must be unit tested for every job.\n",
    "- Jobs will be specified just like any function, with input types and output types\n",
    "    - some of the inputs will be specified as Spark DataFrames, and their schema is part of their type, in an abstract way; the same applies to their outputs\n",
    "    - one advantage of this approach is that general unit testing techniques can be applied, by first writing an specification as a comment in the job's source code, and then systematically synthesising the unit test from the specification (see [MIT Reading 6.005 — Software Construction, 6: Specification](http://web.mit.edu/6.005/www/sp16/classes/06-specifications/))\n",
    "    - so, preconditions, postconditions and effects will be explicitly stated as part of each job's specification\n",
    "    - when a precondition is either expensive or non-trivial to check for the client, an exception will be thrown by the job; otherwise the job's behaviour is undetermined.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d743677",
   "metadata": {},
   "source": [
    "### Data sources\n",
    "\n",
    "Storage\n",
    "```\n",
    "eda    laptop   datalake/eda (if a 100 data scientists, they would need accounts)\n",
    "dev    laptop   datalake/dev\n",
    "test   laptop   datalake/airflow\n",
    "prod   S3       s3:...\n",
    "```\n",
    "\n",
    "```\n",
    "SparkETL('conf_name') <--- use Airflow vars and conns\n",
    "\n",
    "3 different environments + common (use python parseconf, files stored in plugins):\n",
    "- common.conf (implicit, no need to pass, could override with airflow vars)\n",
    "- dev.conf\n",
    "- airflow.conf\n",
    "- cloud.conf\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "        root \n",
    "\n",
    "-eda    -raw    - curated           - staging    - check+load -->  - production\n",
    "        sas     state_ref           immigration                    *_dim\n",
    "        ref     port_ref            *_dim                          *_fact\n",
    "                country_ref         incr_fact\n",
    "                demographics_ref    \n",
    "                airport_ref\n",
    "                port_to_airport_ref\n",
    "                temperature_ref\n",
    "                \n",
    "staging *_ref will be just in memory\n",
    "- when loading to prod, first load dims, then facts\n",
    "```\n",
    "\n",
    "### Extract\n",
    "\n",
    "The source data is extracted as is into the following staging tables:\n",
    "\n",
    "- **immigration_staging**(Unnamed: 0, cicid, i94yr, i94mon, i94cit, i94res, i94port, arrdate, i94mode, i94addr, depdate, i94bir, i94visa, count, dtadfile, visapost, occup, entdepa, entdepd, entdepu, matflag, biryear, dtaddto, gender, insnum, airline, admnum, fltno, visatype)\n",
    "\n",
    "\n",
    "- From immigration data dictionary:\n",
    "    - **i94port_staging**(port_id, port_desc)\n",
    "    - **i94country_staging**(country_id, country_desc)\n",
    "    - **i94mode_staging**(mode_id, mode_desc)\n",
    "    - **i94state_staging**(state_id, state_desc)\n",
    "    - **i94visa_staging**(visa_id, visa_desc)\n",
    "\n",
    "\n",
    "- **airports_staging**(ident, type, name, elevation_ft, continent, iso_country, iso_region, municipality, gps_code, iata_code, local_code, coordinates)\n",
    "\n",
    "\n",
    "- **demographics_staging**(City, State, Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born, Average Household Size, State Code, Race, Count)\n",
    "\n",
    "\n",
    "- **temperatures_staging**(dt, AverageTemperature, AverageTemperatureUncertainty, State, Country)\n",
    "\n",
    "### Transform\n",
    "\n",
    "#### Cleaning\n",
    "\n",
    "The goal is to create views from which the fact and dimension tables can be easily derived, with all data issues solved.\n",
    "\n",
    "The way I understand cleaning data is quite broad: it consists of normalizing, dealing with missing and out of range values, formatting, eliminating duplicates, enriching, verifying and standardizing. The outcome is rectangular tidy data (columns represent variables and rows represent observations), with tables that have only the required columns with labels suitable for the domain at hand, and that can be easily and correctly joined with other clean tables for further processing.\n",
    "\n",
    "standardizing: all tables should refer to the same entities by the same set of values\n",
    "verifying: assert assumptions (like uniqueness)\n",
    "\n",
    "- **immigration**(arrival_date, airline, flight_number, port, citizenship, residence, age, age_group, gender, visa, address_state, stay, stay_group)\n",
    "- **state**(state_id, name, type_id, type)\n",
    "- **port**(port_id, state_id, name)\n",
    "- **country**(country_id, country)\n",
    "- **airport**(airport_id, name, city, state_id, type, coordinates)\n",
    "- **demographics**(state_id, city, asian, black, latino, native, white, ethnicity_id, ethnicity, population, size_id, size)\n",
    "- **temperature**(state_id, climate_id, climate)\n",
    "\n",
    "Principles:\n",
    "- Trim every raw string; annoying bugs can creep in otherwise.\n",
    "\n",
    "#### Generate association tables:\n",
    "\n",
    "The goal is to create tables that allow to easily join clean tables, starting from clean tables.\n",
    "\n",
    "- port_to_airport(port_id, airport_id)\n",
    "\n",
    "#### Insert required rows into dimension tables\n",
    "\n",
    "For each dimension:\n",
    "1. Figure out which rows are missing, by performing an anti join between each dimension's natural keys and immigration.\n",
    "2. For each dimension join those missing natural keys with reference tables to fill in all columns.\n",
    "\n",
    "#### Process fact table\n",
    "\n",
    "With the immigration table:\n",
    "1. Join with dimension tables using their natural keys to get corresponding surrogate keys.\n",
    "2. Group by all dimension natural keys to calculate aggregates.\n",
    "\n",
    "### Load\n",
    "\n",
    "Append."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4b8195",
   "metadata": {},
   "source": [
    "# Scratch area\n",
    "\n",
    "Do the ETL in a Spark cluster in EMR\n",
    "- use the cluster's HDFS for storing intermediate results, so that it is efficient to do the different steps as different Airflow tasks.\n",
    "    - but maybe can just store intermediate results in S3, hopefully data size will be much smaller after the first step, do it this way in the first pass for simplicity\n",
    "    - remember to clean up at the end of the pipeline\n",
    "    - let Spark take care of partitioning\n",
    "\n",
    "### Extract\n",
    "\n",
    "#### Sources\n",
    "- immigration monthly SAS\n",
    "- airports\n",
    "- state temperatures\n",
    "- demographics\n",
    "- i94 data dictionary\n",
    "- airline.dat\n",
    "\n",
    "### Transform\n",
    "\n",
    "##### immigration monthly SAS into flight_fact\n",
    "\n",
    "want ```flight_fact(arrdate, (airline,fltno,port), citizenship, residence, address, passenger_num)```\n",
    "\n",
    "    - filter:\n",
    "        - keep only arrivals by Air\n",
    "    - drop all except for:\n",
    "        - arrdate, airline, fltno, i94cit, i94res, i94port, i94addr\n",
    "    - decode:\n",
    "        - i94port, i94addr, airline, fltno\n",
    "    - rename and cast:\n",
    "        - arrdate as int\n",
    "        - concat(airline,fltno,port) as string   # in a first pass, later maybe md5 % maxint\n",
    "        - i94cit as citizenship int\n",
    "        - i94res as residence int\n",
    "        - i94addr as address int\n",
    "        \n",
    "##### airports\n",
    "\n",
    "want airports_ref(state, city, coordinates)\n",
    "\n",
    "    - filter:\n",
    "        - don't filter by iso_country, this way might keep Guam, etc\n",
    "    - transform:\n",
    "        - iso_region into state\n",
    "        - municipality into city\n",
    "    - drop all except for:\n",
    "        - state, city, coordinates\n",
    "    - group by:\n",
    "        - state, city and just choose any one coordinate\n",
    "\n",
    "        \n",
    "##### state temperatures\n",
    "\n",
    "want temperatures_ref(state, temperature)\n",
    "\n",
    "Questions: \n",
    "- how to handle missing data (i.e. 2016)?\n",
    "    - try to find more recent data source or update this one\n",
    "    - predict with ML\n",
    "    - just impute latest for same month\n",
    "- seems that some ports are not in US states, try to join those too\n",
    "\n",
    "    - filter:\n",
    "        - as above, keep all countries around just in case\n",
    "        - keep only latest year\n",
    "    - rename:\n",
    "        - AverageTemperature to temperature decimal(3,1)\n",
    "        - State to state string\n",
    "    - drop:\n",
    "        - keep state, temperature\n",
    "\n",
    "##### demographics\n",
    "\n",
    "Questions:\n",
    "- how to handle missing years?\n",
    "    - in a first pass, treat it as static (with SCD 0 or 1)\n",
    "- in a first pass, keep just total population\n",
    "\n",
    "##### i94 data dictionary\n",
    "    - create country data frame\n",
    "    \n",
    "##### airline.dat\n",
    "    - create airline data frame\n",
    "\n",
    "#### Dimensions\n",
    "\n",
    "- country_dim\n",
    "\n",
    "#### Facts\n",
    "- from immigration data frame:\n",
    "    - i94cit, i94res, i94\n",
    "\n",
    "### Load\n",
    "\n",
    "- In the cloud\n",
    "    - Prefer a Data Lake instead of Data Warehouse\n",
    "    - The reason is that it is yet unclear how frequently this data will be analysed, so provisioning, say, a Redshift cluster could be overkill, no need to pay to have dedicated computation resources and database managed storage.\n",
    "    - For this reason, blob storage would be preferable\n",
    "    - This way analysts can use any tool they want (Pandas, Databricks, spark cluster or standalone, Athena, viz tools, etc)\n",
    "    - If there is high demand, might consider Redshift later on\n",
    "    - There is no need to stick with Spark, given that data volume is not that high\n",
    "    \n",
    "Save everything in **parquet**, because it is structured and distributed\n",
    "\n",
    "- use fastparquet, pyarrow gives error\n",
    "    - index=False\n",
    "    - book recommeds gzip commpression, but start by using default snappy\n",
    "    - can go into more detail with regards to partitioning for big data case in step 5\n",
    "\n",
    "- flight_fact:\n",
    "    - flight_fact.parquet\n",
    "    - partition_cols=[year, month] #, md5(airline,fltno,port)]\n",
    "    - in a first pass partition just on (year, month)\n",
    "- country_dim:\n",
    "    - country.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7bbd9",
   "metadata": {},
   "source": [
    "## Eating my dog food\n",
    "\n",
    "Perform some analysis using my schema\n",
    "- opportunity to show off my pandas and plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f9efa6",
   "metadata": {},
   "source": [
    "[Nulls in Fact Tables](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/fact-table-null/)\n",
    "\n",
    "> Null-valued measurements behave gracefully in fact tables. The aggregate functions (SUM, COUNT, MIN, MAX, and AVG) all do the “right thing” with null facts. However, nulls must be avoided in the fact table’s foreign keys because these nulls would automatically cause a referential integrity violation. Rather than a null foreign key, the associated dimension table must have a default row (and surrogate key) representing the unknown or not applicable condition.\n",
    "\n",
    "[Null Attributes in Dimensions](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/null-dimension-attribute/)\n",
    "> Null-valued dimension attributes result when a given dimension row has not been fully populated, or when there are attributes that are not applicable to all the dimension’s rows. In both cases, we recommend substituting a descriptive string, such as Unknown or Not Applicable in place of the null value. Nulls in dimension attributes should be avoided because different databases handle grouping and constraining on nulls inconsistently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e2ff1",
   "metadata": {},
   "source": [
    "### SparkSubmitOperator notes\n",
    "\n",
    "https://medium.com/codex/executing-spark-jobs-with-apache-airflow-3596717bbbe3\n",
    "\n",
    "(https://airflow.apache.org/docs/apache-airflow-providers-apache-spark/stable/index.html)\n",
    "- [Install Airflow Spark provider package and PySpark]\n",
    "```\n",
    "conda install -c conda-forge pyspark\n",
    "conda install -c conda-forge apache-airflow-providers-apache-spark FAILED\n",
    "pip install apache-airflow-providers-apache-spark\n",
    "```\n",
    "- you must register the master Spark connection in the Airflow administrative panel.\n",
    "- conda installs pyspark 2.4; latest, 3.2.1, breaks Airflow\n",
    "- as such, it requires java8\n",
    "\n",
    "### Livy notes\n",
    "\n",
    "- [I have tried **Livy** but since it **doesn't support spark version 3**](https://stackoverflow.com/questions/71207654/how-can-we-connect-to-remote-spark-cluster-via-jupyterhub)\n",
    "\n",
    "https://livy.apache.org/get-started/\n",
    "- It is strongly recommended to configure Spark to submit applications in YARN cluster mode. That makes sure that user sessions have their resources properly accounted for in the YARN cluster, and that the host running the Livy server doesn’t become overloaded when multiple user sessions are running.\n",
    "\n",
    "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-livy.html\n",
    "- Can configure an EMR cluster to include a Livy server.\n",
    "\n",
    "https://airflow.apache.org/docs/apache-airflow-providers-apache-livy/stable/_api/airflow/providers/apache/livy/operators/livy/index.html\n",
    "- Airflow Livy provider\n",
    "\n",
    "- DAG example: \n",
    "https://github.com/panovvv/airflow-livy-operators/blob/master/airflow_home/dags/03_batch_example.py\n",
    "- Spark job for example above:\n",
    "https://github.com/panovvv/airflow-livy-operators/blob/master/batches/join_2_files.py\n",
    "- [Is it possible to configure Apache Livy to run with Spark Standalone?](https://stackoverflow.com/questions/40876586/is-it-possible-to-configure-apache-livy-to-run-with-spark-standalone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db925e7c",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb523226",
   "metadata": {},
   "source": [
    "https://www.precisely.com/blog/data-quality/data-validation-vs-data-verification\n",
    "\n",
    "Data validation:\n",
    "- Purpose:\tCheck whether data falls within the acceptable range of values\n",
    "- Usually performed:\tWhen data is created or updated\t\n",
    "- Example:\tChecking whether user-entered ZIP code can be found\n",
    "\n",
    "Data verification:\n",
    "- Check data to ensure it’s accurate and consistent\n",
    "- When data is migrated or merged\n",
    "- Checking that all ZIP codes in dataset are in ZIP+4 format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d8e3f3",
   "metadata": {},
   "source": [
    "Goals: to ensure that data meets requirements\n",
    "\n",
    "- check not empty\n",
    "- check count range\n",
    "- check columns not nulls/nan\n",
    "- check no dups\n",
    "- check column ranges\n",
    "- check integrity (references to other tables)\n",
    "\n",
    "\n",
    "Implementation:\n",
    "- start by creating a class in a python file (ETLValidation) that is called from notebooks\n",
    "- custom plugins in Airflow parametrized with Spark SQL templates\n",
    "- custom Airflow hooks to create parametrized Spark SQL views and execute queries\n",
    "- [raise ValueError](https://stackoverflow.com/questions/54593320/validationerror-or-typeerror-valueerror-exceptions) when validation fails\n",
    "\n",
    "TODO:\n",
    "- draw class diagram of ETLCheck operators, and a diagram of how check jobs are submitted to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33737c90",
   "metadata": {},
   "source": [
    "## Unit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28be91ab",
   "metadata": {},
   "source": [
    "[Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)\n",
    "- You can write unit tests for both your tasks and your DAG.\n",
    "- tasks must be **idempotent**\n",
    "    - should this be a data test or a unit test? I think a unit test, given that it must be true of any particular input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edfb25c",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d28555",
   "metadata": {},
   "source": [
    "- deal with missing data in every dataset\n",
    "- do something interesting with the data\n",
    "- deal with nulls in dimension columns\n",
    "- dimension table partitioning\n",
    "- split port to airport matching in 2 steps, start with exact matching\n",
    "- save intermediate tables, like matching scores, for analysis\n",
    "- user persist where appropriate\n",
    "- validation\n",
    "- unit tests\n",
    "- airflow @daily?\n",
    "- watch ds100 viz\n",
    "- consider applying freq. itemsets: passenger in some flight to some state\n",
    "- integrate airlines.dat\n",
    "- find opportunities for avoiding code duplication\n",
    "- error handling for bad data\n",
    "- deal with warnings in ETL notebooks when saving\n",
    "- airflow: pass and configure args (like dirs) properly\n",
    "- add logging; reduce Spark info\n",
    "- clean i94 address field, dict says it is often invalid\n",
    "- port name can be matched to airport or city (or just do both at once)\n",
    "- viz with nice maps\n",
    "- integrate monthly temperature\n",
    "- explore using routedat \"is_direct\" with residence\n",
    "- explore matching flight_number to routedat\n",
    "- must I always use explicit schemas for reading and writing?\n",
    "- airflow unit (integration?) tests\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
