{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a697d341",
   "metadata": {},
   "source": [
    "# Step 3: Define the Data Model\n",
    "\n",
    "### The data model is organized as a star schema\n",
    "- The goal is to **trade storage space and consistency checks for speed**.\n",
    "- The key is for queries to perform **as few joins as possible**, which are an expensive but unavoidable operation.\n",
    "- For this reason, dimension schemas will contain all columns relevant to the dimension at hand, even if this **information is repeated** in other dimensions, rather than normalizing.\n",
    "\n",
    "### Design principles that I followed\n",
    "- *Pure star schema*, as opposed to a snowflake schema. There will be a lot of repition, for the reason stated above, given that, as it is, storage is not an issue.\n",
    "- I will use a dimension's natural key, if any, as its *primary key* if the key is derived from a single data source. This principle also applies to compound primariy keys (for instance, in the ```route_dim``` table).\n",
    "- All dimension columns whose values are codes or abbreviations will have a corresponding *descriptive column* (with the ```_desc``` suffix).\n",
    "- *No measures in dimension tables*. So, I used demographics and temperature tables only to derive static population, ethnical and weather categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881ad858",
   "metadata": {},
   "source": [
    "## Model description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db3f42c",
   "metadata": {},
   "source": [
    "### Abstract data model\n",
    "\n",
    "I will start by presenting what I consider as the core entities that are relevant to my understanding of this model's domain:\n",
    "\n",
    "![Conceptual Data Model](images/de-capstone-abstract-data-model.png)\n",
    "\n",
    "**NOTE**: I preferred to draw a more abstract and simpler (fewer symbols) conceptual data model diagram instead of the more common E-R diagram, because in my opinion the latter includes detail that in my opinion is unnecessary when one is already set on a star model. The intention is to highlight the main atoms and how they they are related. The multiplicities represent business rules and will be useful later on when designing data validation.\n",
    "\n",
    "### Core entities description\n",
    "\n",
    "I will describe only those entities whose precise meaning might not be obvious.\n",
    "\n",
    "#### Route\n",
    "A particular airline and flight number combination that flies to a given immigration port city.\n",
    "- it is possible for an airplane to call at multiple US airports, but under the same flight number (see [Can two flights of the same airlines have the same number?](https://www.quora.com/Can-two-flights-of-the-same-airlines-have-the-same-number)).\n",
    "\n",
    "#### Flight\n",
    "An airplane on a particular route that arrived at a given immigration port in a given day.\n",
    "\n",
    "#### Foreign Visitor\n",
    "A non-US citizen that arrived in a flight for non-immigration purposes, such as pleasure, business or studies.\n",
    "- it is possible that there actually were other passengers in a flight, US citizens, but we don't have any data about these and are out of scope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940820ea",
   "metadata": {},
   "source": [
    "### Star schema\n",
    "\n",
    "![Star schema](images/de-capstone-star-schema.png)\n",
    "\n",
    "#### Tables\n",
    "\n",
    "##### flight_fact table\n",
    "\n",
    "The only fact table in this model, it contains aggregate measures of a flight's foreign visitors.\n",
    "\n",
    "The main measure, ```num_visitors```, is the count of each of the resulting foreign visitor categories after grouping by different visitor categories.\n",
    "\n",
    "Each ```time_id``` and ```route_id``` combination refers to an individual flight, which may have multiple foreign ```visitor_id```s, one for each passenger category combination in that particular flight.\n",
    "\n",
    "##### foreign_visitor_dim table\n",
    "Dimension table that represents combinations of foreign visitor categories.\n",
    "\n",
    "##### time_dim table\n",
    "\n",
    "Dimension table that represents time with day granularity.\n",
    "\n",
    "##### route_dim table\n",
    "\n",
    "Dimension table that represents a route, with many columns that describe the destination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817fdf08",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8553f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "![Data Flow diagram](images/data_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d743677",
   "metadata": {},
   "source": [
    "### Data sources\n",
    "\n",
    "### Extract\n",
    "\n",
    "The source data is extracted as is into the following staging tables:\n",
    "\n",
    "- **immigration_staging**(Unnamed: 0, cicid, i94yr, i94mon, i94cit, i94res, i94port, arrdate, i94mode, i94addr, depdate, i94bir, i94visa, count, dtadfile, visapost, occup, entdepa, entdepd, entdepu, matflag, biryear, dtaddto, gender, insnum, airline, admnum, fltno, visatype)\n",
    "\n",
    "\n",
    "- From immigration data dictionary:\n",
    "    - **i94port_staging**(port_id, port_desc)\n",
    "    - **i94country_staging**(country_id, country_desc)\n",
    "    - **i94mode_staging**(mode_id, mode_desc)\n",
    "    - **i94state_staging**(state_id, state_desc)\n",
    "    - **i94visa_staging**(visa_id, visa_desc)\n",
    "\n",
    "\n",
    "- **airports_staging**(ident, type, name, elevation_ft, continent, iso_country, iso_region, municipality, gps_code, iata_code, local_code, coordinates)\n",
    "\n",
    "\n",
    "- **demographics_staging**(City, State, Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born, Average Household Size, State Code, Race, Count)\n",
    "\n",
    "\n",
    "- **temperatures_staging**(dt, AverageTemperature, AverageTemperatureUncertainty, State, Country)\n",
    "\n",
    "### Transform\n",
    "\n",
    "#### Cleaning\n",
    "\n",
    "The goal is to create views from which the fact and dimension tables can be easily derived, with all data issues solved.\n",
    "\n",
    "The way I understand cleaning data is quite broad: it consists of normalizing, dealing with missing and out of range values, formatting, eliminating duplicates, enriching, verifying and standardizing. The outcome is rectangular tidy data (columns represent variables and rows represent observations), with tables that have only the required columns with labels suitable for the domain at hand, and that can be easily and correctly joined with other clean tables for further processing.\n",
    "\n",
    "standardizing: all tables should refer to the same entities by the same set of values\n",
    "verifying: assert assumptions (like uniqueness)\n",
    "\n",
    "- **immigration**(arrival_date, airline, flight_number, port, citizenship, residence, age, gender, visa, address_state, stay)\n",
    "- **port**(port, state, name)\n",
    "- **state**(state, name, type)\n",
    "- **country**(country_id, country)\n",
    "- **airport**(airport_id, name, city, state, coordinates)\n",
    "- **city**(state, city, size, ethnicity)\n",
    "- **temperature**(state, month, temperature, climate)\n",
    "\n",
    "#### Generate association tables:\n",
    "\n",
    "The goal is to create tables that allow to easily join clean tables, starting from clean tables.\n",
    "\n",
    "- port_to_city(port_id, state, city)\n",
    "\n",
    "#### Insert required rows into dimension tables\n",
    "\n",
    "For each dimension:\n",
    "1. Figure out which rows are missing, by performing an anti join between each dimension's natural keys and immigration.\n",
    "2. For each dimension join those missing natural keys with reference tables to fill in all columns.\n",
    "\n",
    "#### Process fact table\n",
    "\n",
    "With the immigration table:\n",
    "1. Join with dimension tables using their natural keys to get corresponding surrogate keys.\n",
    "2. Group by all dimension natural keys to calculate aggregates.\n",
    "\n",
    "### Load\n",
    "\n",
    "Append."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4b8195",
   "metadata": {},
   "source": [
    "# Scratch area\n",
    "\n",
    "Do the ETL in a Spark cluster in EMR\n",
    "- use the cluster's HDFS for storing intermediate results, so that it is efficient to do the different steps as different Airflow tasks.\n",
    "    - but maybe can just store intermediate results in S3, hopefully data size will be much smaller after the first step, do it this way in the first pass for simplicity\n",
    "    - remember to clean up at the end of the pipeline\n",
    "    - let Spark take care of partitioning\n",
    "\n",
    "### Extract\n",
    "\n",
    "#### Sources\n",
    "- immigration monthly SAS\n",
    "- airports\n",
    "- state temperatures\n",
    "- demographics\n",
    "- i94 data dictionary\n",
    "- airline.dat\n",
    "\n",
    "### Transform\n",
    "\n",
    "##### immigration monthly SAS into flight_fact\n",
    "\n",
    "want ```flight_fact(arrdate, (airline,fltno,port), citizenship, residence, address, passenger_num)```\n",
    "\n",
    "    - filter:\n",
    "        - keep only arrivals by Air\n",
    "    - drop all except for:\n",
    "        - arrdate, airline, fltno, i94cit, i94res, i94port, i94addr\n",
    "    - decode:\n",
    "        - i94port, i94addr, airline, fltno\n",
    "    - rename and cast:\n",
    "        - arrdate as int\n",
    "        - concat(airline,fltno,port) as string   # in a first pass, later maybe md5 % maxint\n",
    "        - i94cit as citizenship int\n",
    "        - i94res as residence int\n",
    "        - i94addr as address int\n",
    "        \n",
    "##### airports\n",
    "\n",
    "want airports_ref(state, city, coordinates)\n",
    "\n",
    "    - filter:\n",
    "        - don't filter by iso_country, this way might keep Guam, etc\n",
    "    - transform:\n",
    "        - iso_region into state\n",
    "        - municipality into city\n",
    "    - drop all except for:\n",
    "        - state, city, coordinates\n",
    "    - group by:\n",
    "        - state, city and just choose any one coordinate\n",
    "\n",
    "        \n",
    "##### state temperatures\n",
    "\n",
    "want temperatures_ref(state, temperature)\n",
    "\n",
    "Questions: \n",
    "- how to handle missing data (i.e. 2016)?\n",
    "    - try to find more recent data source or update this one\n",
    "    - predict with ML\n",
    "    - just impute latest for same month\n",
    "- seems that some ports are not in US states, try to join those too\n",
    "\n",
    "    - filter:\n",
    "        - as above, keep all countries around just in case\n",
    "        - keep only latest year\n",
    "    - rename:\n",
    "        - AverageTemperature to temperature decimal(3,1)\n",
    "        - State to state string\n",
    "    - drop:\n",
    "        - keep state, temperature\n",
    "\n",
    "##### demographics\n",
    "\n",
    "Questions:\n",
    "- how to handle missing years?\n",
    "    - in a first pass, treat it as static (with SCD 0 or 1)\n",
    "- in a first pass, keep just total population\n",
    "\n",
    "##### i94 data dictionary\n",
    "    - create country data frame\n",
    "    \n",
    "##### airline.dat\n",
    "    - create airline data frame\n",
    "\n",
    "#### Dimensions\n",
    "\n",
    "- country_dim\n",
    "\n",
    "#### Facts\n",
    "- from immigration data frame:\n",
    "    - i94cit, i94res, i94\n",
    "\n",
    "### Load\n",
    "\n",
    "- In the cloud\n",
    "    - Prefer a Data Lake instead of Data Warehouse\n",
    "    - The reason is that it is yet unclear how frequently this data will be analysed, so provisioning, say, a Redshift cluster could be overkill, no need to pay to have dedicated computation resources and database managed storage.\n",
    "    - For this reason, blob storage would be preferable\n",
    "    - This way analysts can use any tool they want (Pandas, Databricks, spark cluster or standalone, Athena, viz tools, etc)\n",
    "    - If there is high demand, might consider Redshift later on\n",
    "    - There is no need to stick with Spark, given that data volume is not that high\n",
    "    \n",
    "Save everything in **parquet**, because it is structured and distributed\n",
    "\n",
    "- use fastparquet, pyarrow gives error\n",
    "    - index=False\n",
    "    - book recommeds gzip commpression, but start by using default snappy\n",
    "    - can go into more detail with regards to partitioning for big data case in step 5\n",
    "\n",
    "- flight_fact:\n",
    "    - flight_fact.parquet\n",
    "    - partition_cols=[year, month] #, md5(airline,fltno,port)]\n",
    "    - in a first pass partition just on (year, month)\n",
    "- country_dim:\n",
    "    - country.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7bbd9",
   "metadata": {},
   "source": [
    "## Eating my dog food\n",
    "\n",
    "Perform some analysis using my schema\n",
    "- opportunity to show off my pandas and plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f9efa6",
   "metadata": {},
   "source": [
    "[Nulls in Fact Tables](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/fact-table-null/)\n",
    "\n",
    "> Null-valued measurements behave gracefully in fact tables. The aggregate functions (SUM, COUNT, MIN, MAX, and AVG) all do the “right thing” with null facts. However, nulls must be avoided in the fact table’s foreign keys because these nulls would automatically cause a referential integrity violation. Rather than a null foreign key, the associated dimension table must have a default row (and surrogate key) representing the unknown or not applicable condition.\n",
    "\n",
    "[Null Attributes in Dimensions](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/null-dimension-attribute/)\n",
    "> Null-valued dimension attributes result when a given dimension row has not been fully populated, or when there are attributes that are not applicable to all the dimension’s rows. In both cases, we recommend substituting a descriptive string, such as Unknown or Not Applicable in place of the null value. Nulls in dimension attributes should be avoided because different databases handle grouping and constraining on nulls inconsistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07d5464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
