{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a697d341",
   "metadata": {},
   "source": [
    "# Step 3: Defien Data model\n",
    "\n",
    "- The system will be used to perform **ad-hoc flight analytics**.\n",
    "- The data model will be organized as a **star schema**.\n",
    "- The goal is to **trade storage space and consistency checks for speed**.\n",
    "- The key is for queries to perform **as few joins as possible**, which are an expensive but unavoidable operation.\n",
    "- For this reason, dimension schemas will contain all columns relevant to the dimension at hand, even if this **information is repeated** in other dimensions, rather than normalizing.\n",
    "- For [**regularly changing dimensions**](https://web.stanford.edu/class/cs345/slides/Lecture5.ppt) (RCDs), such as ```passenger_dim``` (because of US address state temperature) an option would be to create a mini dimension for the state temperatures, but, given that this is an optimization to prevent dimension row explosion, I won't do it for the time being. Same might be applied to state temperature in ```flight_dim```. Avoiding too many optimizations is wise given that this database is meant for early stage ad-hoc analysis.\n",
    "\n",
    "![Star schema](de-capstone-star-schema.png)\n",
    "\n",
    "[Nulls in Fact Tables](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/fact-table-null/)\n",
    "\n",
    "> Null-valued measurements behave gracefully in fact tables. The aggregate functions (SUM, COUNT, MIN, MAX, and AVG) all do the “right thing” with null facts. However, nulls must be avoided in the fact table’s foreign keys because these nulls would automatically cause a referential integrity violation. Rather than a null foreign key, the associated dimension table must have a default row (and surrogate key) representing the unknown or not applicable condition.\n",
    "\n",
    "[Null Attributes in Dimensions](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/null-dimension-attribute/)\n",
    "> Null-valued dimension attributes result when a given dimension row has not been fully populated, or when there are attributes that are not applicable to all the dimension’s rows. In both cases, we recommend substituting a descriptive string, such as Unknown or Not Applicable in place of the null value. Nulls in dimension attributes should be avoided because different databases handle grouping and constraining on nulls inconsistently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940820ea",
   "metadata": {},
   "source": [
    "**flight_fact**(arrdate, flight, passenger, passnum)\n",
    "- passnum is the measure\n",
    "- decided to create separate passenger_dim rather than having different passenger attributes in the fact table\n",
    "\n",
    "**time_dim**((year,month), year, month, day, monthnm)\n",
    "\n",
    "- candidate columns: is weekend, is holiday in US, etc\n",
    "- how about \"is holiday at src country\"?\n",
    "\n",
    "**flight_dim**((airline,fltno,dstport), airline, fltno, dstport, dstairportnum, airlinenm, (more airline info), dstcity, dststate, dststatenm, dstcoord, dstpop, dststatetemp)\n",
    "\n",
    "    - with the given data sources, it is not always possible to identify the exact airport\n",
    "    - it might be possible for cities that have only one international airport\n",
    "    - but I won't attempt this for the time being\n",
    "    - another possibility would be to have a junk dimension with all airports for a given port; its pk would be dstport; don't do this for the time being\n",
    "    - however, a temporary compromise would be add a #international airports at the city column (dstairportnum)\n",
    "\n",
    "**passenger_dim**((citizenship, residence, usaddr, year, month), citizenship, residence, usaddr, year, month, addrtemp, ... more passenger categories ie age, gender, visa, etc)\n",
    "- represents passenger category combinations rather than individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817fdf08",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4b8195",
   "metadata": {},
   "source": [
    "Do the ETL in a Spark cluster in EMR\n",
    "- use the cluster's HDFS for storing intermediate results, so that it is efficient to do the different steps as different Airflow tasks.\n",
    "    - but maybe can just store intermediate results in S3, hopefully data size will be much smaller after the first step, do it this way in the first pass for simplicity\n",
    "    - remember to clean up at the end of the pipeline\n",
    "    - let Spark take care of partitioning\n",
    "\n",
    "### Extract\n",
    "\n",
    "#### Sources\n",
    "- immigration monthly SAS\n",
    "- airports\n",
    "- state temperatures\n",
    "- demographics\n",
    "- i94 data dictionary\n",
    "- airline.dat\n",
    "\n",
    "### Transform\n",
    "\n",
    "##### immigration monthly SAS into flight_fact\n",
    "\n",
    "want ```flight_fact(arrdate, (airline,fltno,port), citizenship, residence, address, passenger_num)```\n",
    "\n",
    "    - filter:\n",
    "        - keep only arrivals by Air\n",
    "    - drop all except for:\n",
    "        - arrdate, airline, fltno, i94cit, i94res, i94port, i94addr\n",
    "    - decode:\n",
    "        - i94port, i94addr, airline, fltno\n",
    "    - rename and cast:\n",
    "        - arrdate as int\n",
    "        - concat(airline,fltno,port) as string   # in a first pass, later maybe md5 % maxint\n",
    "        - i94cit as citizenship int\n",
    "        - i94res as residence int\n",
    "        - i94addr as address int\n",
    "        \n",
    "##### airports\n",
    "\n",
    "want airports_ref(state, city, coordinates)\n",
    "\n",
    "    - filter:\n",
    "        - don't filter by iso_country, this way might keep Guam, etc\n",
    "    - transform:\n",
    "        - iso_region into state\n",
    "        - municipality into city\n",
    "    - drop all except for:\n",
    "        - state, city, coordinates\n",
    "    - group by:\n",
    "        - state, city and just choose any one coordinate\n",
    "\n",
    "        \n",
    "##### state temperatures\n",
    "\n",
    "want temperatures_ref(state, temperature)\n",
    "\n",
    "Questions: \n",
    "- how to handle missing data (i.e. 2016)?\n",
    "    - try to find more recent data source or update this one\n",
    "    - predict with ML\n",
    "    - just impute latest for same month\n",
    "- seems that some ports are not in US states, try to join those too\n",
    "\n",
    "    - filter:\n",
    "        - as above, keep all countries around just in case\n",
    "        - keep only latest year\n",
    "    - rename:\n",
    "        - AverageTemperature to temperature decimal(3,1)\n",
    "        - State to state string\n",
    "    - drop:\n",
    "        - keep state, temperature\n",
    "\n",
    "##### demographics\n",
    "\n",
    "Questions:\n",
    "- how to handle missing years?\n",
    "    - in a first pass, treat it as static (with SCD 0 or 1)\n",
    "- in a first pass, keep just total population\n",
    "\n",
    "##### i94 data dictionary\n",
    "    - create country data frame\n",
    "    \n",
    "##### airline.dat\n",
    "    - create airline data frame\n",
    "\n",
    "#### Dimensions\n",
    "\n",
    "- country_dim\n",
    "\n",
    "#### Facts\n",
    "- from immigration data frame:\n",
    "    - i94cit, i94res, i94\n",
    "\n",
    "### Load\n",
    "\n",
    "- In the cloud\n",
    "    - Prefer a Data Lake instead of Data Warehouse\n",
    "    - The reason is that it is yet unclear how frequently this data will be analysed, so provisioning, say, a Redshift cluster could be overkill, no need to pay to have dedicated computation resources and database managed storage.\n",
    "    - For this reason, blob storage would be preferable\n",
    "    - This way analysts can use any tool they want (Pandas, Databricks, spark cluster or standalone, Athena, viz tools, etc)\n",
    "    - If there is high demand, might consider Redshift later on\n",
    "    - There is no need to stick with Spark, given that data volume is not that high\n",
    "    \n",
    "Save everything in **parquet**, because it is structured and distributed\n",
    "\n",
    "- use fastparquet, pyarrow gives error\n",
    "    - index=False\n",
    "    - book recommeds gzip commpression, but start by using default snappy\n",
    "    - can go into more detail with regards to partitioning for big data case in step 5\n",
    "\n",
    "- flight_fact:\n",
    "    - flight_fact.parquet\n",
    "    - partition_cols=[year, month] #, md5(airline,fltno,port)]\n",
    "    - in a first pass partition just on (year, month)\n",
    "- country_dim:\n",
    "    - country.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7bbd9",
   "metadata": {},
   "source": [
    "## Eating my dog food\n",
    "\n",
    "Perform some analysis using my schema\n",
    "- opportunity to show off my pandas and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1caa80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
